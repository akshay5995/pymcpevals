# PyMCPEvals

> **âš ï¸ Still Under Development** - This project is actively being developed. APIs may change and features are being added. Please use with caution in production environments.

**Evaluation framework for MCP (Model Context Protocol) servers with LLM-based scoring.**

ğŸš€ **Help MCP server developers test their tools by evaluating whether they successfully accomplish user goals.**

## Features

- ğŸ“Š **LLM-based Evaluation**: Uses GPT-4, Claude, Gemini and other models to score server responses
- ğŸ” **Tool Usage Tracking**: Monitor which tools are called and their success/failure rates  
- âš¡ **Single & Multi-turn Testing**: Support both simple prompts and complex conversation trajectories
- ğŸ› ï¸ **FastMCP Integration**: Seamless connection to MCP servers via stdio or HTTP
- ğŸ“‹ **Multiple Output Formats**: Table, detailed, JSON, and JUnit XML for CI/CD

## Quick Start

```bash
# Install
pip install pymcpevals

# Create a template configuration
pymcpevals init

# Edit evals.yaml with your server and test cases
# Run evaluations
pymcpevals run evals.yaml
```

## Simple Example

Create `evals.yaml`:

```yaml
model:
  provider: openai
  name: gpt-4

server:
  command: ["python", "my_server.py"]

evaluations:
  - name: "weather_planning"
    description: "Can users plan their day with weather info?"
    prompt: "What should I wear tomorrow in San Francisco?"
    expected_result: "Should provide weather forecast and clothing suggestions"
    threshold: 3.5
    
  - name: "data_insights" 
    description: "Can users get insights from their database?"
    prompt: "Show me my best performing products this month"
    expected_result: "Should query database and provide ranked product list"
    threshold: 4.0

  - name: "multi_step_weather"
    description: "Test multi-step weather analysis"
    turns:
      - role: "user"
        content: "What's the weather like in London?"
        expected_tools: ["get_weather"]
      - role: "user"
        content: "And how about Paris?"
        expected_tools: ["get_weather"]
    expected_result: "Should provide weather for both cities"
    threshold: 4.0
```

Run evaluations:

```bash
pymcpevals run evals.yaml
```

You'll get output showing:
- âœ…/âŒ Pass/fail status with scores (1-5 scale)
- ğŸ“Š Detailed scores: accuracy, completeness, relevance, clarity, reasoning
- ğŸ”§ Tool usage summary and execution times
- ğŸ’­ LLM judge comments and feedback

## How It Works

PyMCPEvals tests whether your MCP server helps users accomplish their goals:

1. **ğŸ”— Connect** to your MCP server using FastMCP
2. **ğŸ” Discover** available tools from the server  
3. **âš¡ Execute** user prompts and track tool calls
4. **ğŸ“Š Grade** results using LLM judges (1-5 scale)
5. **ğŸ“‹ Report** scores and tool usage details

## Core Problem Solved

**"Do my MCP tools actually help users accomplish their goals?"**

Instead of just testing if your tools work, PyMCPEvals tests if they're **useful**:

- âœ… Can my weather server help someone plan their day?
- âœ… Does my database server help users get insights?  
- âœ… Can my file system server help users organize documents?

## Evaluation Types

### 1. Single-Prompt Evaluations

Test individual prompts to verify basic functionality:

```yaml
evaluations:
  - name: "basic_weather"
    prompt: "What's the weather in Boston?"
    expected_result: "Should call weather API and return current conditions"
    threshold: 3.0
```

### 2. Multi-Turn Trajectories

Test complex conversations with multiple exchanges:

```yaml
evaluations:
  - name: "weather_comparison"
    turns:
      - role: "user"
        content: "What's the weather in Boston?"
        expected_tools: ["get_weather"]
      - role: "user"  
        content: "How does that compare to New York?"
        expected_tools: ["get_weather"]
    expected_result: "Should provide weather for both cities with comparison"
    threshold: 4.0
```

## Installation

```bash
pip install pymcpevals
```

## Usage

### CLI Interface

```bash
# Create template config
pymcpevals init evals.yaml

# Run evaluations
pymcpevals run evals.yaml

# Override server for quick testing
pymcpevals run evals.yaml --server "node server.js"

# Override model 
pymcpevals run evals.yaml --model claude-3-opus-20240229 --provider anthropic

# Parallel execution
pymcpevals run evals.yaml --parallel

# Different output formats
pymcpevals run evals.yaml --output table     # Simple table view
pymcpevals run evals.yaml --output detailed  # Detailed with tool info
pymcpevals run evals.yaml --output json      # Full JSON
pymcpevals run evals.yaml --output junit --output-file results.xml  # CI/CD
```

### Simple Interface

```bash
# Direct evaluation: pymcpevals eval <config> <server>
pymcpevals eval evals.yaml server.py
```

## Configuration

### YAML Configuration

```yaml
# Model configuration
model:
  provider: openai     # openai, anthropic, gemini, etc.
  name: gpt-4         # Model name
  # api_key: ${OPENAI_API_KEY}  # Optional, uses env var

# Server configuration  
server:
  # For local servers (stdio transport)
  command: ["python", "my_server.py"]
  env:
    DEBUG: "true"
    
  # For remote servers (HTTP transport)  
  # url: "https://api.example.com/mcp"
  # headers:
  #   Authorization: "Bearer ${API_TOKEN}"

# Evaluations to run
evaluations:
  - name: "basic_functionality"
    description: "Test core server capabilities"  
    prompt: "What can you help me with?"
    expected_result: "Should describe available tools and capabilities"
    threshold: 3.0  # Minimum score to pass (1-5 scale)
    tags: ["basic"]
    
  - name: "specific_task"
    description: "Test domain-specific functionality"
    prompt: "Help me analyze my sales data for trends"
    expected_result: "Should use appropriate tools to analyze sales data"
    threshold: 3.5
    tags: ["analysis", "data"]

  - name: "multi_step_task"
    description: "Test multi-step problem solving"
    turns:
      - role: "user"
        content: "I need help with my weather data analysis"
        expected_tools: ["get_weather"]
      - role: "user"
        content: "Can you compare today's weather with last week?"
        expected_tools: ["get_weather", "compare_data"]
    expected_result: "Should gather weather data and perform comparison"
    threshold: 4.0
    tags: ["multi-step"]

# Global settings
timeout: 30.0      # Timeout per evaluation
parallel: false    # Run evaluations in parallel
```

### Environment Variables

```bash
# API keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
```

## Server Transport Support

PyMCPEvals uses [FastMCP](https://github.com/jlowin/fastmcp) for server connections:

### Local Servers (Stdio)

```yaml
server:
  command: ["python", "server.py"]
  env:
    DEBUG: "true"
```

### Remote Servers (HTTP)

```yaml
server:
  url: "https://api.example.com/mcp"  
  headers:
    Authorization: "Bearer ${API_TOKEN}"
    Custom-Header: "value"
```

## Example Output

### Table View (--output table)

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Name                        â”ƒ Accuracy â”ƒ Completeness â”ƒ Relevance â”ƒ Clarity â”ƒ Reasoning â”ƒ Average â”ƒ Status â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Weather planning query      â”‚ 4.5      â”‚ 4.0          â”‚ 5.0       â”‚ 4.2     â”‚ 4.1       â”‚ 4.36    â”‚ PASS   â”‚
â”‚ Data analysis task          â”‚ 3.1      â”‚ 3.3          â”‚ 3.8       â”‚ 3.5     â”‚ 3.2       â”‚ 3.38    â”‚ PASS   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Summary: 2/2 passed (100.0%) - Average: 3.87/5.0
```

### Detailed View (--output detailed)

```
MCP Server Evaluation Results

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Test                  â”ƒ Status â”ƒ Scoreâ”ƒ Tools Used         â”ƒ Time   â”ƒ Errors â”ƒ Notes                        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Weather planning      â”‚ PASS   â”‚ 4.36 â”‚ weather_api        â”‚ 245ms  â”‚ 0      â”‚ Good weather integration     â”‚
â”‚ Data analysis         â”‚ PASS   â”‚ 3.38 â”‚ query_db, analyze  â”‚ 891ms  â”‚ 0      â”‚ Retrieved and analyzed data  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Summary: 2/2 passed (100.0%) - Average: 3.87/5.0
```

## Development

```bash
# Install in development mode
git clone https://github.com/akshay5995/pymcpevals
cd pymcpevals
pip install -e ".[dev]"

# Run tests
pytest

# Format code
black src/
ruff check src/
```

## Key Benefits

### For MCP Server Developers
- **ğŸ” Goal-Oriented Testing**: Test if your tools actually help users accomplish tasks
- **âš¡ Tool Usage Insights**: See which tools are used and their success rates
- **ğŸ“Š Detailed Scoring**: 5-dimensional scoring (accuracy, completeness, relevance, clarity, reasoning)
- **ğŸ› ï¸ Easy Integration**: Works with any MCP server via FastMCP

### For Development Teams  
- **ğŸš€ CI/CD Integration**: JUnit XML output for automated testing pipelines
- **ğŸ“ˆ Progress Tracking**: Monitor improvement over time with consistent scoring
- **ğŸ”„ Regression Testing**: Ensure new changes don't break existing functionality
- **âš–ï¸ Model Comparison**: Test across different LLM providers

## Acknowledgments

ğŸ™ **Huge kudos to [mcp-evals](https://github.com/mclenhard/mcp-evals)** - This Python package was heavily inspired by the excellent Node.js implementation by [@mclenhard](https://github.com/mclenhard). The original mcp-evals project pioneered LLM-based evaluation for MCP servers and established many of the patterns and approaches we've adapted for the Python ecosystem.

If you're working in a Node.js environment, definitely check out the original [mcp-evals](https://github.com/mclenhard/mcp-evals) project, which also includes GitHub Action integration and monitoring capabilities.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Ensure all tests pass
5. Submit a pull request

## License

MIT - see LICENSE file.
